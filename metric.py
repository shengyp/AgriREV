# -*- coding: utf-8 -*-
# python metric.py
import os
import json
from collections import defaultdict
from datasets import load_dataset

# è§£å†³predè¢«æˆªæ–­çš„é—®é¢˜
def extract_complete_json_objects(text):
    objs = []
    brace_count = 0
    start = None

    for i, ch in enumerate(text):
        if ch == "{":
            if brace_count == 0:
                start = i
            brace_count += 1
        elif ch == "}":
            brace_count -= 1
            if brace_count == 0 and start is not None:
                obj_str = text[start:i+1]
                try:
                    objs.append(json.loads(obj_str))
                except json.JSONDecodeError:
                    print(f"å­å¯¹è±¡æœ‰æ‹¬å·ï¼Œä½†å†…éƒ¨ç»“æ„é”™è¯¯âŒï¸ï¼š{obj_str}")
                    pass
                start = None
    # print(f"\nğŸ§    æŠ¢æ•‘æˆåŠŸï¼š{objs}")
    return objs


def clean_output(text):
    # å»æ‰ ```json ``` åŒ…è£¹
    cleaned = text.strip()
    if cleaned.startswith("```"):
        cleaned = cleaned.strip("`")
        cleaned = cleaned.replace("json", "", 1).strip()

    return cleaned


# ---------- åŠ è½½ dataset ----------
def load_triples_from_file(file_path: str):
    data_dir = os.path.dirname(file_path)
    data_file = os.path.basename(file_path)

    dct = load_dataset(
        path="json",
        data_dir=data_dir,
        data_files=data_file,
        split="train"
    )

    gold_samples, pred_samples, tasks = [], [], []

    error_output = 0
    for sample in dct:
        gold_str = sample["conversations"][1]["content"]
        pred_str = clean_output(sample["pred"])    # å»æ‰ ```json ``` åŒ…è£¹
        task = sample["task"]
        id = sample["id"]
        if task == "topic":
            gold_json = gold_str.strip() if isinstance(gold_str, str) else ""
            pred_json = pred_str.strip() if isinstance(pred_str, str) else ""
        else:
            try:
                gold_json = json.loads(gold_str)    # è‚¯å®šæ­£ç¡®
                pred_json = json.loads(pred_str)    # å¯èƒ½è§£æé”™è¯¯
            except Exception as e:
                # print(f"\nğŸš€    {id}  éœ€è¦æŠ¢æ•‘ï¼ŒåŸå§‹ pred_strï¼š{pred_str}")
                error_output += 1
                pred_json = extract_complete_json_objects(pred_str)

        gold_samples.append(gold_json)
        pred_samples.append(pred_json)
        tasks.append(task)
        
    print(f"\nä¸èƒ½è§£æçš„æ ·æœ¬æ•°é‡ï¼š{error_output}")
    return gold_samples, pred_samples, tasks



def parse_json_output(text: str):
    """è§£æJSONè¾“å‡º"""
    try:
        if text.strip():
            return json.loads(text)
        return []
    except Exception:
        return []


def compute_metrics(gold_samples, pred_samples, tasks):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    
    results = {}

    # ========= validator =========
    validator_correct, validator_total = 0, 0
    validator_tp = validator_fp = validator_fn = validator_tn = 0

    # ========= topic =========
    topic_correct, topic_total = 0, 0
    topic_tp = topic_fp = topic_fn = topic_tn = 0

    # ========= ner =========
    ner_tp = ner_pred_total = ner_gold_total = 0
    ner_type_stats = defaultdict(lambda: {"tp": 0, "pred": 0, "gold": 0})

    # ========= re =========
    re_tp = re_pred_total = re_gold_total = 0
    re_type_stats = defaultdict(lambda: {"tp": 0, "pred": 0, "gold": 0})

    # ========= é€æ¡è®¡ç®— =========
    for gold, pred, task in zip(gold_samples, pred_samples, tasks):
        if task == "validator":
            validator_total += 1
            pred_text = pred if isinstance(pred, str) else str(pred)
            gold_text = gold if isinstance(gold, str) else str(gold)
            if pred_text.strip() == gold_text.strip():
                validator_correct += 1
            
            if gold == "æ­£ç¡®" and pred == "æ­£ç¡®":
                validator_tp += 1
            elif gold == "é”™è¯¯" and pred == "æ­£ç¡®":
                validator_fp += 1
            elif gold == "æ­£ç¡®" and pred == "é”™è¯¯":
                validator_fn += 1
            elif gold == "é”™è¯¯" and pred == "é”™è¯¯":
                validator_tn += 1

        elif task == "topic":
            topic_total += 1
            pred_text = pred if isinstance(pred, str) else str(pred)
            gold_text = gold if isinstance(gold, str) else str(gold)
            if pred_text.strip() == gold_text.strip():
                topic_correct += 1
            
            if gold == "ç›¸å…³" and pred == "ç›¸å…³":
                topic_tp += 1
            elif gold == "ä¸ç›¸å…³" and pred == "ç›¸å…³":
                topic_fp += 1
            elif gold == "ç›¸å…³" and pred == "ä¸ç›¸å…³":
                topic_fn += 1
            elif gold == "ä¸ç›¸å…³" and pred == "ä¸ç›¸å…³":
                topic_tn += 1

        elif task == "ner":
            pred_json = pred if isinstance(pred, list) else []
            gold_json = gold if isinstance(gold, list) else []

            # è½¬æ¢ä¸ºå®ä½“é›†åˆ
            pred_set = set()
            for e in pred_json:
                if isinstance(e, dict):
                    entity = e.get("entity", "").strip()
                    type = e.get("type", "").strip()
                    if entity and type:
                        pred_set.add(f"{entity}::{type}")
                        ner_type_stats[type]["pred"] += 1

            gold_set = set()
            for e in gold_json:
                if isinstance(e, dict):
                    entity = e.get("entity", "").strip()
                    type = e.get("type", "").strip()
                    if entity and type:
                        gold_set.add(f"{entity}::{type}")
                        ner_type_stats[type]["gold"] += 1

            # è®¡ç®—æ€»ä½“æŒ‡æ ‡
            correct_set = pred_set & gold_set
            ner_tp += len(correct_set)
            ner_pred_total += len(pred_set)
            ner_gold_total += len(gold_set)

            # æŒ‰ç±»å‹ç»Ÿè®¡æ­£ç¡®æ•°
            for entity_str in correct_set:
                type = entity_str.split("::")[1] if "::" in entity_str else ""
                if type:
                    ner_type_stats[type]["tp"] += 1

        elif task == "re":
            pred_json = pred if isinstance(pred, list) else []
            gold_json = gold if isinstance(gold, list) else []

            # è½¬æ¢ä¸ºå…³ç³»é›†åˆ
            pred_set = set()
            for r in pred_json:
                if isinstance(r, dict):
                    head = r.get("head", "").strip()
                    relation = r.get("relation", "").strip()
                    tail = r.get("tail", "").strip()
                    if head and relation and tail:
                        pred_set.add(f"{head}::{relation}::{tail}")
                        re_type_stats[relation]["pred"] += 1

            gold_set = set()
            for r in gold_json:
                if isinstance(r, dict):
                    head = r.get("head", "").strip()
                    relation = r.get("relation", "").strip()
                    tail = r.get("tail", "").strip()
                    if head and relation and tail:
                        gold_set.add(f"{head}::{relation}::{tail}")
                        re_type_stats[relation]["gold"] += 1

            # è®¡ç®—æ€»ä½“æŒ‡æ ‡
            correct_set = pred_set & gold_set
            re_tp += len(correct_set)
            re_pred_total += len(pred_set)
            re_gold_total += len(gold_set)

            # æŒ‰ç±»å‹ç»Ÿè®¡æ­£ç¡®æ•°
            for rel_str in correct_set:
                relation = rel_str.split("::")[1] if "::" in rel_str else ""
                if relation:
                    re_type_stats[relation]["tp"] += 1

    # ===== æ±‡æ€»æŒ‡æ ‡ =====
    
    # validator å‡†ç¡®ç‡
    if validator_total > 0:
        precision = validator_tp / (validator_tp + validator_fp) if validator_tp + validator_fp > 0 else 0.0
        recall = validator_tp / (validator_tp + validator_fn) if validator_tp + validator_fn > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0

        results["validator"] = {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "accuracy": validator_correct / validator_total,
            "correct": validator_correct,
            "total": validator_total,
            "tp": validator_tp,
            "fp": validator_fp,
            "fn": validator_fn,
            "tn": validator_tn
        }


    # Topic å‡†ç¡®ç‡
    if topic_total > 0:
        precision = topic_tp / (topic_tp + topic_fp) if topic_tp + topic_fp > 0 else 0.0
        recall = topic_tp / (topic_tp + topic_fn) if topic_tp + topic_fn > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0

        results["topic"] = {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "accuracy": topic_correct / topic_total,
            "correct": topic_correct,
            "total": topic_total,
            "tp": topic_tp,
            "fp": topic_fp,
            "fn": topic_fn,
            "tn": topic_tn
        }

    # NER æŒ‡æ ‡
    if ner_gold_total > 0:
        # Micro æŒ‡æ ‡
        ner_precision = ner_tp / ner_pred_total if ner_pred_total > 0 else 0.0
        ner_recall = ner_tp / ner_gold_total if ner_gold_total > 0 else 0.0
        ner_f1 = 2 * ner_precision * ner_recall / (ner_precision + ner_recall) if (ner_precision + ner_recall) > 0 else 0.0
        
        results["ner"] = {
            "micro": {
                "precision": ner_precision,
                "recall": ner_recall,
                "f1": ner_f1
            },
            "tp": ner_tp,
            "pred_total": ner_pred_total,
            "gold_total": ner_gold_total
        }
        
        # æ¯ä¸ªå®ä½“ç±»å‹çš„æŒ‡æ ‡
        type_metrics = {}
        for type, stats in ner_type_stats.items():
            if stats["pred"] > 0 or stats["gold"] > 0:
                type_precision = stats["tp"] / stats["pred"] if stats["pred"] > 0 else 0.0
                type_recall = stats["tp"] / stats["gold"] if stats["gold"] > 0 else 0.0
                type_f1 = 2 * type_precision * type_recall / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0.0
                
                type_metrics[type] = {
                    "precision": type_precision,
                    "recall": type_recall,
                    "f1": type_f1,
                    "tp": stats["tp"],
                    "pred": stats["pred"],
                    "gold": stats["gold"]
                }
        
        results["ner"]["per_type"] = type_metrics

    # RE æŒ‡æ ‡
    if re_gold_total > 0:
        # Micro æŒ‡æ ‡
        re_precision = re_tp / re_pred_total if re_pred_total > 0 else 0.0
        re_recall = re_tp / re_gold_total if re_gold_total > 0 else 0.0
        re_f1 = 2 * re_precision * re_recall / (re_precision + re_recall) if (re_precision + re_recall) > 0 else 0.0
        
        results["re"] = {
            "micro": {
                "precision": re_precision,
                "recall": re_recall,
                "f1": re_f1
            },
            "tp": re_tp,
            "pred_total": re_pred_total,
            "gold_total": re_gold_total
        }
        
        # æ¯ä¸ªå…³ç³»ç±»å‹çš„æŒ‡æ ‡
        type_metrics = {}
        for relation, stats in re_type_stats.items():
            if stats["pred"] > 0 or stats["gold"] > 0:
                type_precision = stats["tp"] / stats["pred"] if stats["pred"] > 0 else 0.0
                type_recall = stats["tp"] / stats["gold"] if stats["gold"] > 0 else 0.0
                type_f1 = 2 * type_precision * type_recall / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0.0
                
                type_metrics[relation] = {
                    "precision": type_precision,
                    "recall": type_recall,
                    "f1": type_f1,
                    "tp": stats["tp"],
                    "pred": stats["pred"],
                    "gold": stats["gold"]
                }
        
        results["re"]["per_type"] = type_metrics

    # ç»¼åˆæŒ‡æ ‡
    results["total_samples"] = len(pred_samples)
    results["task_distribution"] = {
        "validator": validator_total,
        "topic": topic_total,
        "ner": sum(1 for t in tasks if t == "ner"),
        "re": sum(1 for t in tasks if t == "re")
    }

    return results


def format_metrics(metrics: dict) -> str:
    """æ ¼å¼åŒ–è¾“å‡ºè¯„ä¼°æŒ‡æ ‡"""
    output = []
    output.append("=" * 50)
    output.append("è¯„ä¼°ç»“æœæ±‡æ€»")
    output.append("=" * 50)

    # validator ç»“æœ
    if "validator" in metrics:
        validator = metrics["validator"]
        output.append(f"\n[validator éªŒè¯]")
        output.append(f"ç²¾ç¡®ç‡: {validator['precision']:.4f}")
        output.append(f"å¬å›ç‡: {validator['recall']:.4f} ")
        output.append(f"F1åˆ†æ•°: {validator['f1']:.4f} ")
        output.append(f"å‡†ç¡®ç‡: {validator['accuracy']:.4f} ({validator['correct']}/{validator['total']})")
        output.append(f"TP: {validator['tp']}, FP: {validator['fp']}, FN: {validator['fn']}, TN: {validator['tn']}")
    
    # Topic ç»“æœ
    if "topic" in metrics:
        topic = metrics["topic"]
        output.append(f"\n[Topic åˆ†ç±»]")
        output.append(f"ç²¾ç¡®ç‡: {topic['precision']:.4f}")
        output.append(f"å¬å›ç‡: {topic['recall']:.4f} ")
        output.append(f"F1åˆ†æ•°: {topic['f1']:.4f} ")
        output.append(f"å‡†ç¡®ç‡: {topic['accuracy']:.4f} ({topic['correct']}/{topic['total']})")
        output.append(f"TP: {topic['tp']}, FP: {topic['fp']}, FN: {topic['fn']}, TN: {topic['tn']}")
    
    # NER ç»“æœ
    if "ner" in metrics:
        ner = metrics["ner"]
        output.append(f"\n[NER å®ä½“è¯†åˆ«]")
        output.append(f"Micro-Precision: {ner['micro']['precision']:.4f}")
        output.append(f"Micro-Recall: {ner['micro']['recall']:.4f}")
        output.append(f"Micro-F1: {ner['micro']['f1']:.4f}")
        output.append(f"æ€»é¢„æµ‹æ•°: {ner['pred_total']}, æ€»æ ‡ç­¾æ•°: {ner['gold_total']}, æ­£ç¡®æ•°: {ner['tp']}")
        
        if ner['per_type']:
            output.append("\næŒ‰å®ä½“ç±»å‹ç»Ÿè®¡:")
            for type, type_metrics in ner['per_type'].items():
                output.append(f"  {type}: P={type_metrics['precision']:.4f}, "
                            f"R={type_metrics['recall']:.4f}, F1={type_metrics['f1']:.4f} "
                            f"(TP={type_metrics['tp']}, P={type_metrics['pred']}, G={type_metrics['gold']})")
    
    # RE ç»“æœ
    if "re" in metrics:
        re = metrics["re"]
        output.append(f"\n[RE å…³ç³»æŠ½å–]")
        output.append(f"Micro-Precision: {re['micro']['precision']:.4f}")
        output.append(f"Micro-Recall: {re['micro']['recall']:.4f}")
        output.append(f"Micro-F1: {re['micro']['f1']:.4f}")
        output.append(f"æ€»é¢„æµ‹æ•°: {re['pred_total']}, æ€»æ ‡ç­¾æ•°: {re['gold_total']}, æ­£ç¡®æ•°: {re['tp']}")
        
        if re['per_type']:
            output.append("\næŒ‰å…³ç³»ç±»å‹ç»Ÿè®¡:")
            for relation, type_metrics in re['per_type'].items():
                output.append(f"  {relation}: P={type_metrics['precision']:.4f}, "
                            f"R={type_metrics['recall']:.4f}, F1={type_metrics['f1']:.4f} "
                            f"(TP={type_metrics['tp']}, P={type_metrics['pred']}, G={type_metrics['gold']})")
    
    output.append(f"\næ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
    output.append(f"ä»»åŠ¡åˆ†å¸ƒ: {metrics['task_distribution']}")
    output.append("=" * 50)
    
    return "\n".join(output)


def main():
    import argparse, os

    parser = argparse.ArgumentParser(description="Evaluation Metrics Script")
    parser.add_argument("--pred", type=str, required=True, help="prediction json file path")
    parser.add_argument("--metric", type=str, required=True, help="metric output json path")

    args = parser.parse_args()

    print("åŠ è½½é¢„æµ‹æ•°æ®...")
    gold_samples, pred_samples, tasks = load_triples_from_file(args.pred)

    assert len(gold_samples) == len(pred_samples), \
        f"æ ·æœ¬æ•°ä¸ä¸€è‡´ï¼š{len(gold_samples)} vs {len(pred_samples)}"


    metrics = compute_metrics(gold_samples, pred_samples, tasks)

    print(format_metrics(metrics))

    os.makedirs(os.path.dirname(args.metric), exist_ok=True)
    with open(args.metric, "w", encoding="utf-8") as f:
        json.dump(metrics, f, ensure_ascii=False, indent=2)

    print(f"\nè¯¦ç»†è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜è‡³: {args.metric}")



if __name__ == "__main__":
    main()