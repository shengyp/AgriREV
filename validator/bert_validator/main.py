# -*- coding:utf-8 -*-

import torch
from utils import read_data, MyDataset
from config import parsers
from torch.utils.data import DataLoader
from model import MyModel
from torch.optim import AdamW
import torch.nn as nn
from sklearn.metrics import accuracy_score
import time
from test import test_data
from tqdm import tqdm
import json
from datetime import datetime
import os
import random
import numpy as np
from transformers import get_linear_schedule_with_warmup

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    # è®© CUDA ç®—å­ç¡®å®šæ€§ï¼ˆé€Ÿåº¦ä¼šç•¥æ…¢ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def write_log(data):
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(json.dumps(data, ensure_ascii=False) + "\n")

if __name__ == "__main__":
    
    set_seed(42)

    start = time.time()
    args = parsers()
    # os.makedirs("logs", exist_ok=True)

    LOG_INTERVAL = args.log_interval    # æ¯å¤šå°‘ä¸ª batch è®°å½•ä¸€æ¬¡
    log_file = args.log_file

    device = "cuda:0" if torch.cuda.is_available() else "cpu"

    train_text, train_label, max_len = read_data(args.train_file)
    dev_text, dev_label = read_data(args.dev_file)
    
    print(f"args.max_len:{args.max_len}")
    # args.max_len = max_len
    print(f"max_len:{max_len}")
    
    train_dataset = MyDataset(train_text, train_label, args.max_len)
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)

    dev_dataset = MyDataset(dev_text, dev_label, args.max_len)
    dev_dataloader = DataLoader(dev_dataset, batch_size=args.batch_size, shuffle=False)

    model = MyModel().to(device)
    opt = AdamW(model.parameters(), lr=args.learn_rate)
    loss_fn = nn.CrossEntropyLoss()

    # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
    total_steps = len(train_dataloader) * args.epochs
    print(f"Total training steps: {total_steps}")
    warmup_steps = int(args.warmup_ratio * total_steps)    # é¢„çƒ­æ­¥æ•°ä¸ºæ€»æ­¥æ•°çš„10%
    print(f"Warmup_steps: {warmup_steps}")
    # åˆ›å»ºè°ƒåº¦å™¨, åœ¨è®­ç»ƒåˆæœŸï¼Œå­¦ä¹ ç‡ä»è¾ƒå°å€¼é€æ¸å¢åŠ ï¼Œé¿å…æ¨¡å‹å› è¾ƒå¤§å­¦ä¹ ç‡è€Œä¸ç¨³å®š
    scheduler = get_linear_schedule_with_warmup(
        opt,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )


    # ===== è®°å½•è®­ç»ƒé…ç½®ï¼ˆåªå†™ä¸€æ¬¡ï¼Œä½œä¸ºç¬¬ä¸€æ¡æ—¥å¿—ï¼‰ =====
    write_log({
        "type": "config",
        "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "seed": 42,
        "device": device,
        "train_size": len(train_text),
        "dev_size": len(dev_text),
        "params": {
            "train_file": args.train_file,
            "dev_file": args.dev_file,
            "test_file": args.test_file,
            "bert_pred": args.bert_pred,
            "class_num": args.class_num,
            "max_len": args.max_len,
            "batch_size": args.batch_size,
            "epochs": args.epochs,
            "learn_rate": args.learn_rate,
            "log_interval": args.log_interval,
            "optimizer": "AdamW",
            "loss": "CrossEntropyLoss"
        }
    })
    
    acc_max = float("-inf")     # å†å²æœ€ä¼˜ dev acc
    early_stop_patience = 6    # è¿ç»­å¤šå°‘ä¸ª epoch æ— æå‡å°±åœæ­¢
    early_stop_counter = 0     # å·²è¿ç»­æ— æå‡çš„ epoch æ•°

    for epoch in range(args.epochs):
        loss_sum, count = 0, 0
        model.train()
        for batch_index, (batch_text, batch_label) in enumerate(
            tqdm(train_dataloader, desc=f"Training-Epoch-{epoch}", total=len(train_dataloader))
        ):
            batch_label = batch_label.to(device)    # æ¯ä¸ª batch = 32 æ¡æ–‡æœ¬ + 32 ä¸ªæ ‡ç­¾
            pred = model(batch_text)    # å‰å‘ä¼ æ’­

            loss = loss_fn(pred, batch_label)    # è®¡ç®—è¿™ä¸€ä¸ªbatchçš„å¹³å‡loss

            # åå‘ä¼ æ’­ä¸‰è¿: æ¸…æ¢¯åº¦, åå‘ä¼ æ’­, å‚æ•°æ›´æ–°
            opt.zero_grad()
            loss.backward()
            opt.step()
            scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡

            loss_sum += loss    # ç´¯ç§¯ loss
            count += 1    # ç´¯ç§¯ batch æ•°

            # ===== æ˜¯å¦éœ€è¦æ‰“å°/è®°å½• =====
            is_log_step = (
                (batch_index + 1) % LOG_INTERVAL == 0
                or (batch_index + 1) == len(train_dataloader)
            )
    
            if is_log_step:
                avg_loss = (loss_sum / count).item()    # è®¡ç®—è¿™ LOG_INTERVAL ä¸ª batch çš„å¹³å‡ loss
    
                msg = "[{0}/{1:5d}]\tTrain_Loss:{2:.4f}"
                # print(msg.format(epoch + 1, batch_index + 1, avg_loss))
    
                write_log({
                    "type": "train",
                    "epoch": epoch + 1,
                    "step": batch_index + 1,
                    "loss": avg_loss,
                    "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                })
    
                loss_sum, count = 0.0, 0

        # æ¯epochéªŒè¯1æ¬¡
        model.eval()
        all_pred, all_true = [], []
        with torch.no_grad():
            for batch_text, batch_label in dev_dataloader:
                batch_label = batch_label.to(device)
                pred = model(batch_text)

                pred = torch.argmax(pred, dim=1).cpu().numpy().tolist()
                label = batch_label.cpu().numpy().tolist()

                all_pred.extend(pred)
                all_true.extend(label)

        acc = accuracy_score(all_true, all_pred)
        print(f"dev acc:{acc:.4f}")
        write_log({
            "type": "dev",
            "epoch": epoch + 1,
            "accuracy": acc,
            "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })
        
        # ===== Early Stopping & Best Model Save =====
        if acc > acc_max:
            print(f"Dev acc improved: {acc_max:.4f} â†’ {acc:.4f}")
            acc_max = acc
            early_stop_counter = 0   # ğŸ”¥ é‡ç½®è®¡æ•°å™¨
        
            torch.save(model.state_dict(), args.save_model_best)
            print("âœ… å·²ä¿å­˜æœ€ä½³æ¨¡å‹")
        
            write_log({
                "type": "checkpoint",
                "epoch": epoch + 1,
                "best_acc": acc,
                "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            })
        else:
            early_stop_counter += 1
            print(
                f"âš ï¸ Dev acc æœªæå‡ï¼ˆ{early_stop_counter}/{early_stop_patience}ï¼‰"
            )
        
            write_log({
                "type": "early_stop_wait",
                "epoch": epoch + 1,
                "wait": early_stop_counter,
                "best_acc": acc_max,
                "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            })
        
            if early_stop_counter >= early_stop_patience:
                print("ğŸ›‘ è§¦å‘ Early Stoppingï¼Œæå‰ç»ˆæ­¢è®­ç»ƒ")
                write_log({
                    "type": "early_stop",
                    "epoch": epoch + 1,
                    "best_acc": acc_max,
                    "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                })
                break

    torch.save(model.state_dict(), args.save_model_last)

    end = time.time()
    print(f"è¿è¡Œæ—¶é—´ï¼š{(end-start)/60%60:.4f} min")

    # ===== æµ‹è¯•é›†è¯„ä¼° + å†™æ—¥å¿— =====
    test_data(log_file=log_file)
